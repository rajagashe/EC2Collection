# Pre-Requisites - Workshop on Recommendation Engine using Apache Spark on Amazon Elastic MapReduce (EMR)
This is the detailed description of the initial setup before attending the workshop on building a Recommendation Engine using Apache Spark running on [Amazon EMR](https://aws.amazon.com/emr/). The workshop uses Python 3, [Zeppelin](https://zeppelin.apache.org/) and the [Spark DataFrames API](https://spark.apache.org/docs/2.1.0/sql-programming-guide.html). Amazon EMR comes with python3 installed on all the cluster nodes, we use emr-5.5.0 release for this workshop, and we configure our EMR cluster to have Zeppelin & Spark installed at launch. Refer to the Cloudformation template for more details.

__NB: You may choose to finish all the steps listed here, except the launching of the cluster until much closer to the time you will be attending the workshop, that way you will not incur unnecessary charges for resources used.__

## Clone this Git repository.

To clone this repository, issue the following command on your command prompt,

`git clone http://github.com/OmarKhayyam/EC2Collection ./my_local_directory`

else, you can use the __Clone or download__ to get the contents of this repository.

Do not change the names of the files, we will soon be uploading them to S3. The Cloudformation template we use will be expecting these exact filenames.

`bootstrap_EMRCluster.sh` - This file sets up the bootstrap_zeppelin.sh to use later.

`bootstrap_zeppelin.sh` - A misnomer actually, this file changes the interpreter we will use by default in the interpreter.json file

`personalRatings.txt` - This is the file that contains a sample set of movies that user zero(0) has rated. To see which movies these are, you can have a look at the rateMovies script. This file is generated by rateMovies script or you can manually edit the ratigs to eeach of the movies.

`rateMovies` - This is the script that is used to rate movies and generate the `personalRatings.txt` file and uploads it to an S3 bucket of your choice. We run this script without any parameters, if you are not able to run this script, just edit `personalRatings.txt` with the ratings for yoru choice for the movies and upload it to the S3 bucket you will create in the next step.

## Creating a bucket
With a name of your choice in the region of your choice. Note, this will decide in which region your EMR cluster will be launched, it will be the same as the region where the bucket is created. You can [create an Amazon S3 bucket using the AWS Management Console](http://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html) or using the CLI, here is an example of creating the bucket using AWS CLI.

`aws s3 mb s3://your-bucket-name`

## Prepare the scripts for use later
Change the following files to use the name of your new bucket instead of the string __myBucket__.

`bootstrap_EMRCluster.sh` - Replace __myBucket__ with the name of the bucket you just created above.

`rateMovies` - Replace __myBucket__ with the name of the bucket you just created above. If you are going to modify the `personalRatings.txt` file by hand, you don't need to do modify this file.

## Getting the Data
We will be using the [Movielens 100K dataset](https://grouplens.org/datasets/movielens/100k/) for this exercise. Download it, unzip the bundle, and upload all the files to the bucket you just created above.

## Uploading the files
We should now upload all of the files we discussed so far, including the Movielens dataset to the newly created S3 bucket. You can either use the CLI to upload your files, like so,

`aws s3 cp sourceFile s3://myBucket`

To use the CLI to upload the files, we will need to ensure that we have our CLI configured, check [here for guidance on how to go about downloading and configuring the AWS CLI](http://docs.aws.amazon.com/cli/latest/userguide/installing.html).

__OR__, 

We could use the AWS Management Console to upload all the files to our bucket, refer to this documentation for [guidance on using the AWS Management Console to upload your files](http://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html).

### At this point, the following files should be in the newly created S3 bucket,

*_bootstrap_EMRCluster.sh_

*_bootstrap_zeppelin.sh_

*_personalRatings.txt_

*_All the files from the Movielens Dataset._

### At this point, you should have the default IAM Roles in place, 

If you are using an IAM User to create these default roles for an Amazn EMR cluster you intend to launch, your IAM user will require the following permissions:

iam:CreateRole

iam:PutRolePolicy

iam:CreateInstanceProfile

iam:AddRoleToInstanceProfile

iam:ListRoles

iam:GetPolicy

iam:GetInstanceProfile

iam:GetPolicyVersion

iam:AttachRolePolicy

iam:PassRole

__Creation of default roles USING THE AWS Management Console__

These IAM roles are expected by the cloudformation template we will use below to spin up our cluster. If you ever launched an Amazon EMR cluster before and used __default roles__ for the cluster, you will have them, the default roles go by the name `EMR_EC2_DefaultRole` and `EMR_DefaultRole`. If you have never launched an EMR cluster in your AWS account, follow the instructions below to ensure that you have created these roles before we use the cloudformation template below,

1. Login to your AWS account and navigate to your IAM Management Console, this is what it will look like,
![](https://github.com/OmarKhayyam/EC2Collection/blob/master/IAMManagementConsole.png?raw=true)

2. Click __Roles__ (highlighted by the rectangle in the screen shot above), this will take you to the __Roles__ screen, click on __Create new role__ at the top, you will be taken to a screen (see below), where you need to choose __AWS Service Role__  as highlighted and choose the role type, in our case, first select the role type named __Amazon Elastic MapReduce__ (we will talk about the other role type shortly).
![](https://github.com/OmarKhayyam/EC2Collection/blob/master/ChooseServiceRole.png?raw=true)

3. This will take you to the Attach Policy screen, refer below screen shot, choose the policy and click __Next step__.
![](https://github.com/OmarKhayyam/EC2Collection/blob/master/AttachPolicy.png?raw=true)

4. In the next screen, give this role the name __EMR_DefaultRole__ and click the __Create role__ button at the bottom of your screen.

5. Follow the steps 2 through 4 for the other role, for the role type, choose __Amazon Elastic MapReduce for EC2__ (ref. screen shot in point 2 above). Same as in point 3 above, you will be presented with a single policy, select the policy, same as you did in point 3 above, and finally, name the role __EMR_EC2_DefaultRole__ and click __Create role__.

__Creation of default roles USING THE AWS CLI__

If you have all your access key and scret keys in plaace and configured AWS CLI on your laptop/machine/desktop, all you have to do to create these roles is issue the following command:

`aws emr create-default-roles`

## Run the Cloudformation template
Now, open the Cloudformation console in the AWS Management Console, and create the stack, the template will ask for some parameters, make sure you already have them, they are,

__EC2 Keypair name__ : Read [here](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html) how to create an EC2 key pair. This will be used to set up SSH between your laptop/desktop to the Master node.

__Location of the bootstrap actions script__ : This is the same as `s3://your-bucket-name/bootstrap_EMRCluster.sh`. You will replace __your-bucket-name__ with the name of the S3 bucket you created above.

__Log URI__ : This is the location where EMR will store the logs, this is useful later for debugging issues you may face. This should be of the format like `s3://myBucket/EMRLogs`, as you can see, you may use prefixes anywhere and everywhere you see fit.

The cloudformation template that we need to run is part of this Git repository, the filename is __launchclusterV2.template__. 

1. Proceed to the __Cloudformation__ service from the AWS Management Console.
2. Click __Create Stack__.
3. On the next page, choose __Upload a template to Amazon S3__. Browse to select the dowloaded file.
4. Provide a __Stack name__ and then provide all the required parameters the template requires to create the stack.
5. Click __Next__ and then review.
6. Click __Create__.

## After the stack is created
We will need to modify the Security group of the master node in the EMR cluster. You can find the Master node's security group by looking at the Cluster details and identifying the master node's security group. Like so,

![](https://github.com/OmarKhayyam/EC2Collection/blob/master/SGandFQDN.png?raw=true)

Click on the security group and add your custom IP address for SSH into the inbound list of allowed IP addresses and ports, for details have a look at [this](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html).

### Setting the Apache Zeppelin interpreter
Issue the following command from your laptop/machine, note that the key path and name are fictitious and should be replaced with your own key path and name, this is the key pair we generated when we created the EC2 key pair (see above). Also, the EC2 fully qualified name for the master node is also fictitious and should be replaced with your own. Refer screenshot above for details on where you can find this information.

`ssh -i ~/Keys/MyEMRKey.pem hadoop@ec2-11-111-111-11.ap-south-1.compute.amazonaws.com 'sudo /home/hadoop/bootstrap_zeppelin.sh'`

#### Windows Users
Log into the master node with putty. [Read instructions to change the .pem file format to putty compatible format here](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html). 

Use putty to login to the Master node, after you have logged in, in the putty terminal issue a command as follows,

`sudo /home/hadoop/bootstrap_zeppelin.sh`

### Setting up the SSH tunnel from your machine/laptop to the Master Node in the EMR cluster
To do this, if you have a Mac or Linux based laptop/machine, issue the following command,

`ssh -f -N -i <full path to the .pem file you got when you generated your EC2 key pair> -L 127.0.0.1:8890:127.0.0.1:8890 hadoop@<Full DNS name of the EMR cluster master node>`

for example, note that all DNS names and key names are fictitious.

`ssh -f -N -i ~/Keys/MyEMRKey.pem -L 127.0.0.1:8890:127.0.0.1:8890 hadoop@ec2-11-111-111-11.ap-south-1.compute.amazonaws.com`

#### Windows Users
To set up an SSH tunnel from your Windows laptop/machine to the Master node in the EMR cluster, [follow these instructions](http://realprogrammers.com/how_to/set_up_an_ssh_tunnel_with_putty.html).

### Accessing the Zeppelin Notebook
All you have to do is open your favourite browser and enter the following in it and hit <ENTER>,

`http://localhost:8890`

This should display a page similar to this,

![](https://github.com/OmarKhayyam/EC2Collection/blob/master/Zeppelin-First-Screen.png?raw=true)
